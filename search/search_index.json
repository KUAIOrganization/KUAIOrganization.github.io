{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"KU AI Club","text":"<p>KU AI Organization will be project-driven and members are encouraged to collaborate and share their knowledge of computer science, mathematics, engineering, finance, natural sciences, and any other relevant fields to develop projects of their choosing. </p> <p>In addition to providing a space to work on interesting AI/ ML projects with other students, this organization will provide opportunities to compete in machine learning competitions and find opportunities to learn more from an industry perspective through public webinars and speaker events. Depending on interest we will also host competitions with other clubs. </p> <p>Students of all expertise levels, majors, and years are welcome!</p>"},{"location":"leaders.html","title":"Club Executives","text":"Spencer Weishaar President Vatsa Pandey Vice-President John Rader Treasurer"},{"location":"projects.html","title":"Projects","text":""},{"location":"resources.html","title":"Competition and Development resources","text":"<p>Honorary mention to Huggingface, a platform where you can store an infinite amount of models and data</p>"},{"location":"resources.html#neural-network-development-resources","title":"Neural Network Development resources","text":"<p>Resources here will be model based, Ideally take a look at the tutorials before coming here</p>"},{"location":"resources.html#transformer-models","title":"Transformer models","text":""},{"location":"resources.html#diffusion-models","title":"Diffusion models","text":""},{"location":"resources.html#competitions","title":"Competitions","text":"<p>The most common location for every competition, Kaggle</p> <p>Here's a couple designed to get you learning the format and maybe explore new kinds of NNs and data science you haven't seen before:</p> <ul> <li>MNIST Data digit recognization, basically the very first useable NN anyone makes, but you can also explore the code and discussions to figure out how people optimize every aspect of a NN</li> <li>Titanic competition, basically the main intro to kaggle</li> <li>Housing prices, more around data science and non NN methods, but will definitely teach you that a NN is not suitable for every situation, and there are much better and lighter options out there</li> <li>Space titanic, Titanic 2, try it after you're done with the first one</li> </ul> <p>Still learner based competitions, but these are either meant for learning specific NNs or specific hardware</p> <ul> <li>LLM classification, a starting point for anyone wanting to classify complex text </li> <li>I\u2019m Something of a Painter Myself, If you're interested in AI generated imagery of any kind, this is your starting point</li> <li>Petals to the Metal, an intro to TPU chips, which google has in massive supply and provides large amounts to for student projects (look up google TRC), good to know how to use their hardware for when you need to scale up a project</li> </ul>"},{"location":"tutorials.html","title":"Tutorials","text":""},{"location":"tutorials.html#a-general-intro-to-machine-learning","title":"A general intro to machine learning","text":"<p>getting the basics down -&gt; Making your first neural network</p>"},{"location":"tutorials.html#looking-at-your-problem-as-a-shape","title":"Looking at your problem as a shape","text":"<p>quickly turning your problem into a model</p> <p>We've come across a variety of people who have a problem they would like to solve that they think AI can help with.</p> <p>A couple examples of this are:</p> <ul> <li>estimating the weight of a fish based off of a screenshot</li> <li>optimizing a power grid to predict what areas will need more power in the next few hours</li> <li>attempting to solve 2d puzzles or other arcade games</li> </ul> <p>Most people come in with the assumption that they will need a completely custom neural network and a lot of knowledge to make the smallest of demos, but in reality, most problems can fit within the scope of common model types.</p>"},{"location":"tutorials.html#start-with-representing-the-problem","title":"Start with representing the problem","text":"<p>Think of how you can represent your data, what shapes seem the most natural or obvious, then consult the table below</p> model type what the input looks like how you usually prepare the data cnn a grid/array of values (like a table or an image) arrange data into rows/columns, add depth if needed mlp a simple list of numbers, or a spreadsheet put everything into a list, scale values rnn/lstm/gru a sequence of steps (like words in a sentence, or time series) make all sequences the same length, turn words/items into numbers transformer a sequence with positions (like a sentence where order matters) same as above, but also give the model info about position/order autoencoder anything, but input and output have the same shape clean/normalize data so it can be reconstructed gnn (graph) a set of points with connections (a network/graph) describe which points are linked and what each point's values are <p>Links to each NN:</p> <ul> <li>cnn tutorial</li> <li>mlp tutorial</li> <li>rnn tutorial (WIP)</li> <li>transformer tutorial (WIP)</li> <li>autoencoder tutorial (WIP)</li> <li>gnn tutorial (WIP)</li> </ul>"},{"location":"tutorials.html#other-approaches","title":"Other Approaches","text":"<p>If you are currently unsure about the shape method above, and want something simpler, use the examples below, but we do recommend the shape method as its more flexible and leads you to to try new things</p> <p>if none of these sound similar to your usecase, bring it up during one of our meetings or email me</p> <ul> <li>CNNs - Image classification, basically anything you can turn into an image (fraud heatmaps, audio spectograms)</li> <li>MLPs - Classifying off of tables, regression</li> <li>RNN/lstm/gru - Basically any time related predictions (t1-&gt;t2-&gt;t3), or basic language modelling (small sentences, words, verbs, etc)</li> <li>Transformer - can basically model any n -&gt; {all previous Ns} sets, but mostly used for language modelling and anything to do with language</li> <li>Autoencoders - compression, anomaly detection (if it can't reconstruct the signal, the signal is outside the usual bounds)</li> <li>GNNs - can model anything representable as a graph or connected points (social networks, databases, 3d models, atoms/molecules)</li> </ul>"},{"location":"tutorials/cnn.html","title":"Making a Convolutional Neural Network, a CNN","text":"<p>this is a simplified intro to cnn\u2019s, enough to understand how they \"see\" images, which is there most common usecase.</p> <p>to those of you who have come from the data shapes section, a CNN will work on any 2d array, but will also perform better if the neighbors in the 2d array are related. An Image is literally a 2d array (every pixel is an rgb list, pixels from a 2d list), and nearby pixels are related (either an object or a boundary), so their most common usecase is image classification, but any 2d array works</p>"},{"location":"tutorials/cnn.html#step-0-prereqs-and-theory","title":"Step 0: prereqs and theory","text":"<p>This is a convolutional neural network, if you want to learn more about convolutions and how they work, check out But what is a convolution? by 3b1b</p> <p>This is a Full Pytorch based tutorial, so you have to get used to pytorch data handling</p> <p>For this tutorial we will be using the MNIST dataset (https://en.wikipedia.org/wiki/MNIST_database), its literally 0-9 written by hand, so its a task about classifying handwritten numbers. </p> <p>Its relatively popular, importing it in is simple</p> <pre><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\n\ntrain = DataLoader(datasets.MNIST('.', train=True, download=True, transform=transforms.ToTensor()), batch_size=32, shuffle=True)\n</code></pre> <p>What does this codeblock do?</p> <p>First we have our imports:</p> <ul> <li><code>torch</code> - Our actual machine learning library, this is where all our functions and classes come from</li> <li><code>nn</code> - technically a part of torch, the most commonly used one, named for convenience</li> <li><code>optim</code> - technically a part of torch, we've named this one for convenience, better than writing <code>torch.optim</code> everywhere, used for the optimizer in your training</li> <li><code>torchvision</code> - This is pytorch's vision toolkit, basically any function that helps out with vision can be found here, along with common vision datasets. transforms are a major feature as well, they are transforms for images, you can Random Crop, Random Flip, or even Color Jitter</li> <li><code>torch.utils.data</code>, <code>DataLoader</code> - relates to file data and checkpoint saving, relevant later</li> </ul> <p>From here we actually load our data</p> <p>A dataloader is a class designed to stream your data to a model in training.</p> <p>With a proper machine learning model, you will usually have an 80/10/10 split of the data, known as Train/Val/Test. Train is the data the model actually sees, the data the model learns from. Val is a part of the data the model never sees, but is tested against while training. This helps look into issues like \"is my model memorizing part of the data\" or \"is my model actually learning patterns, and can it solve out of distribution (outside of train) questions?\". Test is another part of the data the model never sees, but isnt really tested against until after training and you've picked your model weights. This is to prevent the model from getting to used to the val data, as many professional workflows will use early stopping, or val loss trends, to continue or stop training a model, which might make your model fit to val as well. With test splits you can prevent this issue.</p> <p>A general tip for data splits is to make sure the splits are uniform, if a split is too different from the other splits, ex. train is 90% blue images but test is 90% red images, comparing the two no longer gives you accurate results, and ruins the point of having splits. Ideally if train is 90% blue and 10% red, val/test should also be 90% blue 10% red.</p> <p>As this is a beginner tutorial, we will only be using the train split.</p> <p><code>download=True</code> is there so you actually get the data, and transforms here is just <code>transforms.ToTensor()</code>, which is just turning your data into a tensor, a tensor is basically just a \"machine learning list\", like imagine a python list but for ML, and thats a tensor. Using <code>transforms.ToTensor()</code> is mostly the same as using <code>int(input())</code>, just making sure its the right format.</p> <p>here we only have the one transform so its a one-liner, but usually you would do something like this:</p> <pre><code>your_transforms = transforms.Compose([\n    transforms.Resize((256, 256)),  # Resize image to 256x256 pixels\n    transforms.RandomCrop(224),     # Randomly crop a 224x224 region\n    transforms.RandomHorizontalFlip(), # Randomly flip horizontally\n    ...\n    transforms.ToTensor(),          # Convert PIL Image or NumPy array to PyTorch Tensor\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # Normalize pixel values\n])\n\ntrain = DataLoader(datasets.MNIST( ..., transform=your_transforms), ...)\n</code></pre> <p><code>batch_size</code> is just how many samples you are sending to the model at once, usually more is better, keep it going till you CPU/GPU runs out of memory or you no longer see performance gains <code>shuffle</code> randomizes the order, generally an improvement to model training, if you want to keep the order the same use <code>torch.manual_seed(&lt;any_number&gt;)</code></p>"},{"location":"tutorials/cnn.html#step-1-mapping-out-a-cnn","title":"Step 1: Mapping out a CNN","text":"<p>now that you've got the data loading, we need to design the model that data has been loaded for</p> <p>The CNN architecture looks like this:</p> <p></p> <p>we take the input, apply a convolution, add a non-linearity, and repeat for a couple blocks, before adding the classification block, where we flatten everything into a giant row, and use a linear layer to morph that into our 10 possible choices</p>"},{"location":"tutorials/cnn.html#step-2-the-convolution-block","title":"Step 2: The convolution block","text":"<p>Since this is a smaller neural network we wont literally make a block, but with larger neural nets you can define a class <code>CNN_block</code> and just call it with <code>CNN_block()</code></p> <p>our Conv block has 3 parts,  - taking an input  - applying a convolution  - adding a non-linearity</p> <p>As our model uses the image tensor the entire way, and a convolution may change the tensor, but not change the tensor datatype, we can use a single conv as the intake and transform</p> <p>A convolution itself is made of kernel_size, stride, and padding. A kernel is basically a matrix, and the kernel size is the size of the matrix. A kernel usually takes a group of values, multiplies it by its values, and returns a value, those values are usually specified outside of machine learning, like a kernel whose values average pixels to blur an image, or specific values to sharpen an image, etc. In a CNN, this kernel is filled with learnable values, values the model changes for itself while training to be any of the patterns mentioned above, and more. Often times in more complicated models, there are multiple stacked kernels, which allow for complex changes that pull out the most important features in a model.</p> <p>The filter will slide around the image, like below:</p> <p></p> <p>The stride and padding both affect this slide. The stride affects how much the slide moves by, like in the image below, where if stride is higher, the kernel averages the same pixels less and also shrinks the input more. </p> <p>Stride:</p> <p></p> <p>Padding is useful when you want to keep the input the same size as before, as you've seen in the first image, things like 5x5 -&gt; 3x3 happen</p> <p>padding adds a ghost border for the conv to use, like effectively doing a 5x5 -&gt; 7x7 -&gt; 5x5</p> <p></p> <p>In pytorch, you can make a conv with <code>nn.conv2d(...)</code>, which defines the learnable filters. These learnable filters are controlled with channels, using <code>in_channel</code> and <code>out_channel</code>, the first two parameters of the conv2d</p> <p>channels are like color channels or filters, like for an input of a grayscale image, you have 1 channel, while an RGB image has 3. From here you use them to expand into your learnable filters, while using the third argument, <code>kernel_size</code> to control the size of the filter</p> <p>knowing that we have grayscale images, lets say I want a convolution to go from 1 channel to 8 filters, each a 3x3, I would have code like this: <code>nn.conv2d(1,8,3)</code>  you can also use stride and padding here, like this: <code>nn.conv2d(1,8,3, stride=..., padding=...)</code> but beware, this will change your tensor size, so you will need to compensate for that.</p> <p>we can use a quick <code>ReLU()</code> for the nonlinearity, use better functions if you'd like.</p> <p>From here I'd like to introduce the <code>nn.MaxPool2d()</code>, similar to the kernels, this instead picks the maximum value from its range, effectively consolidating the most important features, while also shrinking the input.</p> <p>To chain conv layers, the next conv's in channels must always match the previous conv's out layers, and generally increasing the models channels by powers of 2 (x2, x4, 8, etc), is best</p> <p>So far the model looks like this (an nn.Sequential stores multiple components to make 1 big network):</p> <pre><code>model = nn.Sequential(\n    nn.Conv2d(1, 8, 3), nn.ReLU(), nn.MaxPool2d(2,2),\n    nn.Conv2d(8, 16, 3), nn.ReLU(), nn.MaxPool2d(2,2),\n)\n</code></pre>"},{"location":"tutorials/cnn.html#step-3-flattening-and-classifying","title":"Step 3: Flattening and classifying","text":"<p>This step is relatively simple compared to the last one, we now only need to flatten the layer and make a linear layer from this flattened array to our choices. A linear layer is great at mapping linear inputs to outputs, but needs that flat layer first.</p> <p>add an <code>nn.flatten()</code> to our nn.Sequential. To get the output, just print the shape from a dummy input, a quick test trick, like this:</p> <pre><code>...model...\n\ndummy_input = torch.randn(1, 1, 28, 28)\n\nprint(model(dummy_input).shape)\n</code></pre> <p>knowing the output, make a linear layer from that value to 10, the number of choices it can be.</p> <p><code>nn.Linear(16*5*5, 10)  # 10 digits</code></p> <p>Now heres the full model:</p> <pre><code>model = nn.Sequential(\n    nn.Conv2d(1, 8, 3), nn.ReLU(), nn.MaxPool2d(2,2),\n    nn.Conv2d(8, 16, 3), nn.ReLU(), nn.MaxPool2d(2,2),\n    nn.Flatten(),\n    nn.Linear(16*5*5, 10)  # 10 digits\n)\n</code></pre>"},{"location":"tutorials/cnn.html#step-4-the-training-loop","title":"step 4: the training loop","text":"<p>from here we add a loss and an optimizer, using cross entropy loss, this is basically the loss you use whenever you have multiple choices, like our 10 classes, compared to something like MSE for accuracy</p> <pre><code>loss_fn = nn.CrossEntropyLoss()\nopt = optim.Adam(model.parameters(), lr=1e-3)\n</code></pre> <p>I encourage you to try out different optimizers, and maybe try learning the very basics, as thats math you encounter from calc 1 to calc 3, the rules in calc 1-3 can be used to make backprop and parts of optimizers</p> <p>from here we define a basic training loop, 1 epoch, predicting, measuring loss, zero-ing your gradients (resets gradients), and performing backpropogation, then repeating the step again</p> <pre><code># training loop\nfor epoch in range(1):\n    for X, y in train:\n        pred = model(X)\n        loss = loss_fn(pred, y)\n        opt.zero_grad()\n        loss.backward()\n        opt.step()\n    print(\"epoch done, loss:\", loss.item())\n</code></pre>"},{"location":"tutorials/cnn.html#conclusion","title":"conclusion","text":"<p>The full code:</p> <pre><code>import torch.optim as optim\n\n# dataset: MNIST digits (images + labels)\nfrom torchvision import datasets, transforms\ntrain = torch.utils.data.DataLoader(\n    datasets.MNIST('.', train=True, download=True,\n                   transform=transforms.ToTensor()),\n    batch_size=32, shuffle=True)\n\n# model + loss + optimizer\nmodel = nn.Sequential(\n    nn.Conv2d(1, 8, 3), nn.ReLU(), nn.MaxPool2d(2,2),\n    nn.Conv2d(8, 16, 3), nn.ReLU(), nn.MaxPool2d(2,2),\n    nn.Flatten(),\n    nn.Linear(16*5*5, 10)\n)\nloss_fn = nn.CrossEntropyLoss()\nopt = optim.Adam(model.parameters(), lr=1e-3)\n\n# training\nfor epoch in range(1):\n    for X, y in train:\n        pred = model(X)\n        loss = loss_fn(pred, y)\n        opt.zero_grad()\n        loss.backward()\n        opt.step()\n    print(\"epoch done, loss:\", loss.item())\n</code></pre> <p>The code should take around 30s to train the full model, reaching a loss of around 0.09</p> <p>If you'd like to learn about maximising performance on this dataset and more modern methods that work on this dataset, take a look at Deep Neural Nets: 33 years ago and 33 years from now</p>"},{"location":"tutorials/cnn.html#extra-saving-and-sharing-weights","title":"extra: saving and sharing weights","text":"<p>as your model starts to get more complex and more useful, you'll want to be able to save and share the model, to use in an app/website, or measure the performance of different versions.</p> <p>Saving a model looks like this, add it after the training loop:</p> <pre><code>torch.save(model.state_dict(), \"cnn_mnist.pth\")\nprint(\"model saved.\")\n</code></pre> <p>That saves the model <code>cnn_mnist</code> in the current directory, as a pickle file, which is like an exe in the way that its stored as binary, but its basically pythons way to save information as a binary.</p> <p>inferencing, aka using the model, looks like this:</p> <pre><code>import torch\nimport torch.nn as nn\nfrom torchvision import transforms\nfrom PIL import Image\n\n# define the same architecture\nmodel = nn.Sequential(\n    nn.Conv2d(1, 8, 3), nn.ReLU(), nn.MaxPool2d(2,2),\n    nn.Conv2d(8, 16, 3), nn.ReLU(), nn.MaxPool2d(2,2),\n    nn.Flatten(),\n    nn.Linear(16*5*5, 10)\n)\n\n# load weights\nmodel.load_state_dict(torch.load(\"cnn_mnist.pth\"))\nmodel.eval()  # set to inference mode\n\n# preprocessing: same as training\ntransform = transforms.Compose([\n    transforms.Grayscale(num_output_channels=1),\n    transforms.Resize((28,28)),\n    transforms.ToTensor()\n])\n\n# load an image (example: \"digit.png\"), should be the same size as the dataset, 28x28\nimg = Image.open(\"digit.png\")\nx = transform(img).unsqueeze(0)  # add batch dim\n\n# predict\nwith torch.no_grad():\n    logits = model(x) # logits is basically the probabilty of each choice\n    pred_class = torch.argmax(logits, dim=1).item()\n\nprint(\"predicted digit:\", pred_class)\n</code></pre>"},{"location":"tutorials/mlp.html","title":"Making a Multi-Layer Perceptron (MLP)","text":"<p>this is a simplified intro to MLPs, enough to understand how they process structured/tabular data. their most common use case is classification or regression on feature vectors (lists of numbers), like predicting if a student passes an exam based on hours studied and hours slept (this time we can do it over 100s of students rather than 1 guy).</p> <p>mlps can technically take any 1d array as input \u2014 unlike cnn\u2019s, they don\u2019t exploit spatial locality, so neighbors in the input don\u2019t matter as much.</p>"},{"location":"tutorials/mlp.html#step-0-prereqs-and-theory","title":"Step 0: prereqs and theory","text":"<p>an MLP is just a standard feedforward neural network. the input goes through fully connected layers (linear transformations) interleaved with non-linearities.</p> <p>if you want a deeper understanding, check out Neural Networks from Scratch.</p> <p>we will use pytorch from the start. first, import what we need:</p> <pre><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\n</code></pre>"},{"location":"tutorials/mlp.html#step-1-preparing-the-data","title":"Step 1: preparing the data","text":"<p>suppose we want to predict if a student passes (<code>1</code>) or fails (<code>0</code>) based on hours studied and hours slept. our dataset could look like this:</p> <pre><code>X = torch.tensor([\n    [5.0, 7.0],\n    [1.0, 2.0],\n    [10.0, 6.0],\n    [2.0, 8.0]\n])\ny = torch.tensor([\n    [1.0],\n    [0.0],\n    [1.0],\n    [0.0]\n])\n</code></pre> <p><code>X</code> is our input (2 features per student), <code>y</code> is the target, now with 4 students.</p> <p>batch size is optional here, but for more data, wrap it in a <code>DataLoader</code> like with cnn\u2019s.</p>"},{"location":"tutorials/mlp.html#step-2-building-the-mlp","title":"Step 2: building the MLP","text":"<p>an MLP consists of:</p> <ul> <li>fully connected (<code>Linear</code>) layers \u2014 these map inputs to outputs</li> <li>activation functions \u2014 like ReLU or Sigmoid</li> </ul> <p>The architecture looks like this:</p> <p></p> <p>here\u2019s a minimal 1-hidden-layer MLP:</p> <pre><code>model = nn.Sequential(\n    nn.Linear(2, 4),  # input layer \u2192 hidden layer\n    nn.ReLU(),        # activation\n    nn.Linear(4, 1),  # hidden layer \u2192 output layer\n    nn.Sigmoid()      # squash into 0\u20131 probability\n)\n</code></pre> <ul> <li>input layer: 2 features \u2192 hidden layer: 4 neurons</li> <li>output layer: 1 neuron \u2192 probability of passing</li> </ul> <p>you can test it with a dummy input:</p> <pre><code>dummy = torch.tensor([[3.0, 5.0]])\nprint(model(dummy))\n</code></pre>"},{"location":"tutorials/mlp.html#step-3-defining-loss-and-optimizer","title":"Step 3: defining loss and optimizer","text":"<p>for binary classification, we can use <code>BCELoss</code> (Binary Cross Entropy), basically cross entropy for two choices</p> <pre><code>loss_fn = nn.BCELoss()\nopt = optim.Adam(model.parameters(), lr=0.01)\n</code></pre> <p>learning rate is higher here because the network is tiny.</p>"},{"location":"tutorials/mlp.html#step-4-the-training-loop","title":"Step 4: the training loop","text":"<p>like cnn\u2019s: forward \u2192 loss \u2192 backprop \u2192 update</p> <pre><code>for epoch in range(1000):\n    pred = model(X)\n    loss = loss_fn(pred, y)\n    opt.zero_grad()\n    loss.backward()\n    opt.step()\n    if epoch % 100 == 0:\n        print(f\"epoch {epoch}, loss: {loss.item():.4f}\")\n</code></pre> <p>after training, the model will output probabilities close to 0 or 1.</p>"},{"location":"tutorials/mlp.html#step-5-saving-and-loading-the-model","title":"Step 5: saving and loading the model","text":""},{"location":"tutorials/mlp.html#saving-weights","title":"saving weights:","text":"<pre><code>torch.save(model.state_dict(), \"mlp_student.pth\")\nprint(\"model saved.\")\n</code></pre>"},{"location":"tutorials/mlp.html#inferencing-later","title":"inferencing later:","text":"<pre><code># define the same architecture\nmodel = nn.Sequential(\n    nn.Linear(2, 4),\n    nn.ReLU(),\n    nn.Linear(4, 1),\n    nn.Sigmoid()\n)\n\n# load weights\nmodel.load_state_dict(torch.load(\"mlp_student.pth\"))\nmodel.eval()\n\n# predict new student\nnew_student = torch.tensor([[6.0, 5.0]])\nwith torch.no_grad():\n    prob = model(new_student).item()\n    print(\"pass probability:\", prob)\n</code></pre>"},{"location":"tutorials/mlp.html#step-6-intuitiona-way-to-think-about-it","title":"Step 6: intuition/a way to think about it","text":"<ul> <li>each layer linearly combines inputs via weights/biases \u2192 applies non-linearity \u2192 passes forward</li> <li>hidden layer \u201clearns features\u201d from raw input</li> <li>output layer maps to final prediction probability</li> </ul>"},{"location":"tutorials/mlp.html#full-code-put-together","title":"Full code (put together)","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# data\nX = torch.tensor([[5.0, 7.0], [1.0, 2.0], [10.0, 6.0], [2.0, 8.0]])\ny = torch.tensor([[1.0], [0.0], [1.0], [0.0]])\n\n# model\nmodel = nn.Sequential(\n    nn.Linear(2, 4),\n    nn.ReLU(),\n    nn.Linear(4, 1),\n    nn.Sigmoid()\n)\n\n# loss + optimizer\nloss_fn = nn.BCELoss()\nopt = optim.Adam(model.parameters(), lr=0.01)\n\n# training\nfor epoch in range(1000):\n    pred = model(X)\n    loss = loss_fn(pred, y)\n    opt.zero_grad()\n    loss.backward()\n    opt.step()\n    if epoch % 100 == 0:\n        print(f\"epoch {epoch}, loss: {loss.item():.4f}\")\n\n# save\ntorch.save(model.state_dict(), \"mlp_student.pth\")\nprint(\"model saved.\")\n\n# inference\nmodel.load_state_dict(torch.load(\"mlp_student.pth\"))\nmodel.eval()\nnew_student = torch.tensor([[6.0, 5.0]])\nwith torch.no_grad():\n    print(\"pass probability:\", model(new_student).item())\n</code></pre>"},{"location":"tutorials/nn_base.html","title":"Making your first Neural Network","text":"<p>This is a simplified intro, enough to get you to understand optimization and how the learning process is affected, for a deeper explanation, look into Neural Networks from Scratch in Python</p> <p></p> <p>This is the simplest form of Neural Network. </p> <p>Input \u2192 Processing \u2192 Output</p> <p>You might notice that this is really similar to a mathematical function X \u2192 Y with a formula in the middle. </p> <p>A Neural Network splits this into three sections:</p> <ul> <li>input layer: where you feed in numbers (your data).  </li> <li>hidden layers: where the math happens (processing).  </li> <li>output layer: where you get predictions/answers.</li> </ul>"},{"location":"tutorials/nn_base.html#step-1-input-layer","title":"step 1: input layer","text":"<p>let's say you want to predict whether a student passes or fails based on how many hours they studied and how much they slept.</p> <p>example input: - hours studied = <code>5</code> - hours slept = <code>7</code></p> <p>we put that into a list:</p> <pre><code>inputs = [5, 7]\n</code></pre>"},{"location":"tutorials/nn_base.html#step-2-weights-and-bias","title":"step 2: weights and bias","text":"<p>a neural network wont just take the input directly, it uses weights and biases to adjust them. Weights are a way to scale the number, a way to see how \"important\" each input is, and a bias is a single number that is used to shift the inputs</p> <p>Here we just use random starting values:</p> <pre><code>weights = [0.1, 0.2]\nbias = 0.5\n</code></pre>"},{"location":"tutorials/nn_base.html#step-3-the-dot-product","title":"step 3: the dot product","text":"<p>we need to get the dot product of the inputs and the weights, shifted by the bias. You've probably seen a 2x2 dot product in linear algebra or basic calc</p> <p></p> <p>In practice, it looks like this:</p> <pre><code>output = (inputs[0] * weights[0]) + (inputs[1] * weights[1]) + bias\nprint(output)\n</code></pre>"},{"location":"tutorials/nn_base.html#step-4-the-activation-function","title":"step 4: the activation function","text":"<p>if we stop here, the network is just doing linear math. to make it handle complex patterns, we apply a non-linear function called an activation function.</p> <p>There are many non-linear activation functions, but the one we are using here is ReLU, which looks like this:</p> <p>ReLU(x) = max(0, x)</p> <p></p> <p>In my personal experience the SiLU function works best, often a 5-10% performance jump on relu, it looks like this:</p> <p></p> <p>we can add the relu function to our code like this:</p> <pre><code>def relu(x):\n    return max(0, x)\n\nactivated_output = relu(output)\nprint(activated_output)\n</code></pre>"},{"location":"tutorials/nn_base.html#step-5-the-output-layer","title":"step 5: the output layer","text":"<p>since we have a pass/fail situation, we have two outputs, represented by 0 and 1, to force our outputs in the 0 to 1 range, we can use a sigmoid function, which looks like this:</p> <p>we can use it in our code through the math library:</p> <pre><code>import math\n\ndef sigmoid(x):\n    return 1 / (1 + math.exp(-x))\n\nfinal_output = sigmoid(output)\nprint(final_output)\n</code></pre> <p><code>final_output</code> will be between 0 and 1, like a probability (ex. 0.8 \u2192 80% chance of passing)</p> <p>putting it all together for now with a couple variable name changes to represent the diagram, gives us the structure of the neural network</p> <pre><code>import math\n\n# inputs\ninputs = [5, 7]\n\n# weights and bias (randomly chosen for now)\nweights = [0.1, 0.2]\nbias = 0.5\n\n# step 1: linear combination\noutput = (inputs[0] * weights[0]) + (inputs[1] * weights[1]) + bias\n\n# step 2: activation\ndef relu(x):\n    return max(0, x)\n\nhidden = relu(output)\n\n# step 3: output activation (sigmoid)\ndef sigmoid(x):\n    return 1 / (1 + math.exp(-x))\n\nfinal_output = sigmoid(hidden)\nprint(\"prediction:\", final_output)\n</code></pre>"},{"location":"tutorials/nn_base.html#step-6-actually-training-the-neural-net","title":"step 6: actually training the neural net","text":"<p>rn the weights/bias are random, so predictions are useless. training = adjusting them until outputs match the right answers.</p> <p>process:</p> <p>give the network data (inputs), compare output with the correct answer (error), adjust weights/bias a little to reduce error, repeat this thousands of times.</p> <p>this is called gradient descent + backpropagation.</p> <p>we wont be diving into the calculus here, as its a more advanced topic, but you can find a detailed breakdown here</p> <p>The loss plot usually looks like this as the NN tries to reach a loss of zero:</p> <p></p> <p>how does the NN know how to move in the right direction to reduce loss?</p> <p>when the NN makes a prediction, theres a certain amount of error between it and the true known answer, we get the mean squared error using code like this:</p> <p><code>MSE = (truth-pred)**2</code></p> <p>This forms a chart that looks like this:</p> <p></p> <p>the closer you are to the true answer, the lower on the graph you'll be.</p> <p>this learning process moves the neural networks weights in a direction, positive/negative, and that amount is scaled by the <code>learning_rate</code> or <code>lr</code>, which is a number, usually between <code>1e-3</code> and <code>1e-5</code>, that makes sure we dont update to much too fast</p> <p>When you move too slow, you'll end up taking too long, wasting expensive GPU time when working with larger models:</p> <p></p> <p>On the other hand, move too fast, and you worsen the model:</p> <p></p>"},{"location":"tutorials/nn_base.html#wrapping-up","title":"wrapping up","text":"<p>we are using pytorch, a standard NN library, for the final code, simply because the backpropagation involved in training is complicated and time consuming, and torch will handle that for us here</p> <p>the final code looks like this:</p> <pre><code>import torch\nimport torch.nn as nn\n\n# dataset (hours studied, hours slept) \u2192 pass/fail\nX = torch.tensor([[5.0, 7.0], [1.0, 2.0], [10.0, 6.0]])\ny = torch.tensor([[1.0], [0.0], [1.0]])\n\n# model\nmodel = nn.Sequential(\n    nn.Linear(2, 1),   # 2 inputs \u2192 1 output\n    nn.Sigmoid()       # squash into 0\u20131\n)\n\n# loss + optimizer\nloss_fn = nn.MSELoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.1) # higher here because we are a simple, tiny neural network \n\n# training loop\nfor epoch in range(100):\n    y_pred = model(X)\n    loss = loss_fn(y_pred, y)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\nprint(\"prediction for [5,7]:\", model(torch.tensor([5.0,7.0])))\n</code></pre>"}]}